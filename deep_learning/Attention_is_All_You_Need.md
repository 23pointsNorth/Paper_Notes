# Attention Is All You Need

Proposes the *Transformer* network architecture for seq2seq tasks, which
outperforms RNNs and CNNs (convs=over time, so can be for non-image based data).
It uses several attention mechanisms and replicated layers (with skip
connections).

Fortunately, Google's implementation is open source so I'll just use that if I
need it.

(Didn't get details on how this really works, it's a bit hard to do so without
actually implementing it.)
